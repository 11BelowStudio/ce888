{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import traceback\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal, Final, List, TypeVar, Dict, Tuple, Any, Optional, Iterable, Union\n",
    "import sklearn as skl\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "dataset: Literal[\"ihdp\",\"jobs\"] = \"ihdp\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#print(sns.plotting_context())\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\",context=\"paper\")\n",
    "\n",
    "import matplotlib.style\n",
    "if \"seaborn-darkgrid\" in matplotlib.style.available:\n",
    "    matplotlib.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_seed: Final[int] = 42\n",
    "\"Using the meaning of life, the universe, and everything as the seed for RNG\"\n",
    "\n",
    "def rng() -> np.random.Generator:\n",
    "    \"\"\"\n",
    "    Creates a new numpy random generator with a seed of 42.\n",
    "    :return: a new numpy random generator with a seed of 42\n",
    "    \"\"\"\n",
    "    return np.random.default_rng(seed=_seed)\n",
    "\n",
    "def rng_state() -> np.random.RandomState:\n",
    "    \"\"\"\n",
    "    Creates a new numpy randomstate with a seed of 42\n",
    "    :return: a new numpy randomstate with a seed of 42\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(seed=_seed)\n",
    "\n",
    "\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IHDP dataset processing\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [],
   "source": [
    "def turn_01_columns_into_int(\n",
    "        dataframe_to_edit: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds all of the columns that just contain values of 0 and 1,\n",
    "    and converts all of those columns to ints.\n",
    "\n",
    "    Dataframe will have an '01' and 'not_01' attr added to it.\n",
    "    Labels for series that only contain values 0 and 1 will be in the '01' tuple\n",
    "    Labels for every other series will be in the 'not_01' tuple\n",
    "\n",
    "    MODIFIES THE GIVEN DATAFRAME!\n",
    "    :param dataframe_to_edit: the dataframe that is being edited\n",
    "    :return: The modified dataframe.\n",
    "    DOES NOT COPY THE GIVEN ORIGINAL DATAFRAME.\n",
    "\n",
    "    >>> import pandas as pd\n",
    "    >>> print(pd.__version__)\n",
    "    1.4.1\n",
    "    >>> before: pd.DataFrame = pd.DataFrame.from_dict(data={\"int01\":[0,1,1,0],\"flt01\":[0.0, 1.0, 0.0, 1.0], \"intNo\": [-1,0,1,2], \"fltNo\":[-1.0, 0.0, 1.0, 2.0], \"intNan\": [0,1,None,0], \"fltNan\":[0.0,1.0,None,0.0]})\n",
    "    >>> before_types = before.dtypes.values\n",
    "    >>> after: pd.DataFrame = turn_01_columns_into_int(before.copy())\n",
    "    >>> after_types = after.dtypes.values\n",
    "    >>> print(after_types[0])\n",
    "    uint8\n",
    "    >>> print(after_types[1])\n",
    "    uint8\n",
    "    >>> print(f\"{before_types[2] == after_types[2]} {before_types[3] == after_types[3]} {before_types[4] == after_types[4]} {before_types[5] == after_types[5]}\")\n",
    "    True True True True\n",
    "    >>> print(f\"{after.attrs['01']}\")\n",
    "    ('int01', 'flt01')\n",
    "    >>> print(f\"{after.attrs['not_01']} \")\n",
    "    ('intNo', 'fltNo', 'intNan', 'fltNan')\n",
    "    \"\"\"\n",
    "    cols_01: List[str] = []\n",
    "    not_01:  List[str] = []\n",
    "    for c in dataframe_to_edit.columns:\n",
    "        #if dataframe_to_edit[c].dtype == np.uint8:\n",
    "        #    continue\n",
    "        if dataframe_to_edit[c].isin([0,1]).all():\n",
    "            dataframe_to_edit[c] = dataframe_to_edit[c].astype(np.uint8)\n",
    "            cols_01.append(c)\n",
    "        else:\n",
    "            not_01.append(c)\n",
    "    dataframe_to_edit.attrs[\"01\"] = tuple(cols_01)\n",
    "    dataframe_to_edit.attrs[\"not_01\"] = tuple(not_01)\n",
    "    return dataframe_to_edit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "data": {
      "text/plain": "         x0        x1        x2        x3        x4        x5  x6  x7  x8  x9  \\\n0  1.397395  0.996346 -1.105624 -0.879606  0.308569 -1.023402   1   0   0   0   \n1  0.269033  0.196818  0.383828  0.161703 -0.629189  1.460832   1   0   1   0   \n2  1.051537  1.795874 -1.105624  0.161703 -0.629189  0.963985   1   0   1   1   \n3  0.662446  0.196818 -0.733261 -0.879606  0.371086 -0.692171   1   0   0   0   \n4  0.856992  1.795874  0.011465 -0.879606  0.558638  0.301522   0   1   1   0   \n\n   ...  x21  x22  x23  x24  t        yf       ycf       ite        t0  \\\n0  ...    0    0    0    1  1  4.771232 -0.298509  4.657928 -0.298509   \n1  ...    0    0    0    0  0  2.956273  5.783770  3.428604  2.956273   \n2  ...    0    0    0    1  0  4.164164  7.055789  3.658195  4.164164   \n3  ...    0    0    0    0  1  6.172307  1.379697  4.585505  1.379697   \n4  ...    0    0    0    0  1  7.834469  2.747986  4.265591  2.747986   \n\n         t1  \n0  4.771232  \n1  5.783770  \n2  7.055789  \n3  6.172307  \n4  7.834469  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>x4</th>\n      <th>x5</th>\n      <th>x6</th>\n      <th>x7</th>\n      <th>x8</th>\n      <th>x9</th>\n      <th>...</th>\n      <th>x21</th>\n      <th>x22</th>\n      <th>x23</th>\n      <th>x24</th>\n      <th>t</th>\n      <th>yf</th>\n      <th>ycf</th>\n      <th>ite</th>\n      <th>t0</th>\n      <th>t1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.397395</td>\n      <td>0.996346</td>\n      <td>-1.105624</td>\n      <td>-0.879606</td>\n      <td>0.308569</td>\n      <td>-1.023402</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4.771232</td>\n      <td>-0.298509</td>\n      <td>4.657928</td>\n      <td>-0.298509</td>\n      <td>4.771232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.269033</td>\n      <td>0.196818</td>\n      <td>0.383828</td>\n      <td>0.161703</td>\n      <td>-0.629189</td>\n      <td>1.460832</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.956273</td>\n      <td>5.783770</td>\n      <td>3.428604</td>\n      <td>2.956273</td>\n      <td>5.783770</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.051537</td>\n      <td>1.795874</td>\n      <td>-1.105624</td>\n      <td>0.161703</td>\n      <td>-0.629189</td>\n      <td>0.963985</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4.164164</td>\n      <td>7.055789</td>\n      <td>3.658195</td>\n      <td>4.164164</td>\n      <td>7.055789</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.662446</td>\n      <td>0.196818</td>\n      <td>-0.733261</td>\n      <td>-0.879606</td>\n      <td>0.371086</td>\n      <td>-0.692171</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6.172307</td>\n      <td>1.379697</td>\n      <td>4.585505</td>\n      <td>1.379697</td>\n      <td>6.172307</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.856992</td>\n      <td>1.795874</td>\n      <td>0.011465</td>\n      <td>-0.879606</td>\n      <td>0.558638</td>\n      <td>0.301522</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7.834469</td>\n      <td>2.747986</td>\n      <td>4.265591</td>\n      <td>2.747986</td>\n      <td>7.834469</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ihdp_full: pd.DataFrame = turn_01_columns_into_int(\n",
    "    pd.read_csv(f\"{dataset}_full.csv\")\n",
    ")\n",
    "\"The full IHDP dataset (with supplementary t0 and t1 info) as a dataframe\"\n",
    "\n",
    "ihdp_full.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 747 entries, 0 to 746\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x0      747 non-null    float64\n",
      " 1   x1      747 non-null    float64\n",
      " 2   x2      747 non-null    float64\n",
      " 3   x3      747 non-null    float64\n",
      " 4   x4      747 non-null    float64\n",
      " 5   x5      747 non-null    float64\n",
      " 6   x6      747 non-null    uint8  \n",
      " 7   x7      747 non-null    uint8  \n",
      " 8   x8      747 non-null    uint8  \n",
      " 9   x9      747 non-null    uint8  \n",
      " 10  x10     747 non-null    uint8  \n",
      " 11  x11     747 non-null    uint8  \n",
      " 12  x12     747 non-null    uint8  \n",
      " 13  x13     747 non-null    uint8  \n",
      " 14  x14     747 non-null    uint8  \n",
      " 15  x15     747 non-null    uint8  \n",
      " 16  x16     747 non-null    uint8  \n",
      " 17  x17     747 non-null    uint8  \n",
      " 18  x18     747 non-null    uint8  \n",
      " 19  x19     747 non-null    uint8  \n",
      " 20  x20     747 non-null    uint8  \n",
      " 21  x21     747 non-null    uint8  \n",
      " 22  x22     747 non-null    uint8  \n",
      " 23  x23     747 non-null    uint8  \n",
      " 24  x24     747 non-null    uint8  \n",
      " 25  t       747 non-null    uint8  \n",
      " 26  yf      747 non-null    float64\n",
      " 27  ycf     747 non-null    float64\n",
      " 28  ite     747 non-null    float64\n",
      " 29  t0      747 non-null    float64\n",
      " 30  t1      747 non-null    float64\n",
      "dtypes: float64(11), uint8(20)\n",
      "memory usage: 78.9 KB\n"
     ]
    }
   ],
   "source": [
    "ihdp_full.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihdp_factuals: pd.DataFrame = ihdp_full.loc[:, ~ihdp_full.columns.isin(\n",
    "    [\"ycf\",\"ite\",\"t0\",\"t1\"]\n",
    ")]\n",
    "\"A version of the IHDP dataset containing ONLY the factual data\"\n",
    "\n",
    "#ihdp_factuals_no_y: pd.DataFrame = ihdp_factuals.loc[:, ihdp_factuals.columns != \"yf\"]\n",
    "\"IHDP dataset with the factual Y omitted\"\n",
    "\n",
    "ihdp_factuals_y: pd.DataFrame = ihdp_factuals.loc[:, ihdp_factuals.columns == \"yf\"]\n",
    "\"Only the Y data from the IHDP dataset\"\n",
    "\n",
    "\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "#ihdp_learn_validation_skf: StratifiedKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=_seed)\n",
    "#\"Using this to remove 10% of the treated/untreated factuals from ihdp for use as part of the validation dataset later on\"\n",
    "\n",
    "#ihdp_learn_indices, ihdp_validation_indices = [i for i in ihdp_learn_validation_skf.split(ihdp_factuals, ihdp_factuals[\"t\"])][0]\n",
    "\n",
    "ihdp_learn_full_df, ihdp_validation_full_df = train_test_split(\n",
    "    ihdp_full,\n",
    "    test_size=0.1,\n",
    "    random_state=rng_state(),\n",
    "    shuffle=True,\n",
    "    stratify=ihdp_full[\"t\"]\n",
    ")\n",
    "\n",
    "ihdp_learn_df: pd.DataFrame = ihdp_learn_full_df.loc[:, ~ihdp_learn_full_df.columns.isin(\n",
    "    [\"ycf\",\"ite\",\"t0\",\"t1\"]\n",
    ")]\n",
    "\"The dataframe that is the subset of the IHDP factual data which will be used for learning feature importances etc\"\n",
    "\n",
    "ihdp_learn_df_x: pd.DataFrame = ihdp_learn_df.loc[:, ihdp_learn_df.columns != \"yf\"]\n",
    "\"X/T info for the dataframe that is the subset of the IHDP factual data which will be used for learning feature importances etc\"\n",
    "ihdp_learn_df_y: pd.DataFrame = ihdp_learn_df.loc[:, ihdp_learn_df.columns == \"yf\"]\n",
    "\"Y info for the dataframe that is the subset of the IHDP factual data which will be used for learning feature importances etc\"\n",
    "\n",
    "ihdp_learn_ite: pd.DataFrame = ihdp_learn_full_df.loc[:,ihdp_learn_full_df.columns==\"ite\"]\n",
    "\n",
    "ihdp_validation_factual_df: pd.DataFrame = ihdp_validation_full_df.loc[:, ~ihdp_validation_full_df.columns.isin(\n",
    "    [\"ycf\",\"ite\",\"t0\",\"t1\"]\n",
    ")]\n",
    "ihdp_validation_factual_df_x: pd.DataFrame = ihdp_validation_factual_df.loc[:, ihdp_validation_factual_df.columns != \"yf\"]\n",
    "ihdp_validation_factual_df_y: pd.DataFrame = ihdp_validation_factual_df.loc[:, ihdp_validation_factual_df.columns == \"yf\"]\n",
    "\n",
    "\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._validation import NotFittedError\n",
    "from sklearn.base import RegressorMixin, TransformerMixin\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from math import inf\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "R = TypeVar('R', bound=RegressorMixin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [],
   "source": [
    "def np_data_and_targets(\n",
    "        df: pd.DataFrame,\n",
    "        targetname: str = \"yf\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts dataframe into a couple of numpy ndarrays for the data without the labels,\n",
    "    and the labels by themselves.\n",
    "    :param df: the Dataframe\n",
    "    :param targetname: The name of the column holding the targets\n",
    "    :return: tuple of [ndarray of the values without the targets, just the class labels]\n",
    "    \"\"\"\n",
    "\n",
    "    inputs:  np.ndarray = df.loc[:,df.columns != targetname].to_numpy()\n",
    "    outputs: np.ndarray =  df.loc[:,targetname].to_numpy()\n",
    "\n",
    "    return inputs, outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def halving_grid_searcher(\n",
    "        regressor: R,\n",
    "        param_grid: Dict[str, List[Any]],\n",
    "        train_data: np.ndarray,\n",
    "        train_targets: np.ndarray,\n",
    "        k_folds: Union[KFold, Iterable[Tuple[np.ndarray, np.ndarray]]] = KFold(n_splits=5, shuffle=False),\n",
    "        class_weights: Optional[np.ndarray] = None,\n",
    "        resource: str = \"n_samples\"\n",
    ") -> HalvingGridSearchCV:\n",
    "\n",
    "    pipe: Pipeline = Pipeline([\n",
    "        (\"scaler\", QuantileTransformer(output_distribution=\"normal\")),\n",
    "        (\"imputer\",KNNImputer(add_indicator=False, weights=\"distance\")),\n",
    "        (\"regressor\",regressor)\n",
    "    ])\n",
    "\n",
    "    n_splits: int = k_folds.get_n_splits() if isinstance(k_folds, skl.model_selection.BaseCrossValidator) else len(k_folds)\n",
    "\n",
    "    n_max_resources: int = train_targets.size if resource==\"n_samples\" else pipe.get_params(deep=True)[resource]\n",
    "\n",
    "\n",
    "\n",
    "    h_grid_search: HalvingGridSearchCV = HalvingGridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        factor=4,\n",
    "        cv=k_folds,\n",
    "        scoring=make_scorer(r2_score),\n",
    "        refit=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        aggressive_elimination=True,\n",
    "        error_score=-1000000000000,\n",
    "        resource=resource,\n",
    "        max_resources= n_max_resources,\n",
    "        min_resources= n_max_resources//4\n",
    "        # I wanted to make this error score negative infinity, however, doing so caused a lot of\n",
    "        # particularly unsightly warning messages to appear.\n",
    "\n",
    "        # So, to save everyone involved from having to look at at a buttload of them with a buttload of numbers in them,\n",
    "        # I'm just setting this to an incredibly low finite number which should be rather hard to reach.\n",
    "        # And if this score (or an even lower score) somehow is reached legitimately, chances are that\n",
    "        # the legitimate score being lower than the error score will be the least of one's concerns.\n",
    "    )\n",
    "\n",
    "    if class_weights is not None:\n",
    "\n",
    "        h_grid_search.fit(\n",
    "            train_data, train_targets, sample_weight=class_weights\n",
    "        )\n",
    "    else:\n",
    "        h_grid_search.fit(\n",
    "            train_data, train_targets\n",
    "        )\n",
    "\n",
    "    return h_grid_search\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [],
   "source": [
    "def nested_halving_grid_searcher(\n",
    "        regressor: R,\n",
    "        param_grid: Dict[str, List[Any]],\n",
    "        learn_data: np.ndarray,\n",
    "        learn_targets: np.ndarray,\n",
    "        kfold_splits: int = 6,\n",
    "        learn_classes: Optional[np.ndarray] = None,\n",
    "        using_class_weights: bool = False,\n",
    "        nested_rng_generator: Optional[np.random.RandomState] = None,\n",
    "        resource: str = \"n_samples\"\n",
    ") -> Dict[HalvingGridSearchCV, float]:\n",
    "\n",
    "    if nested_rng_generator is None:\n",
    "        nested_rng_generator = rng_state()\n",
    "\n",
    "    h_grid_search_dicts: Dict[HalvingGridSearchCV, float] = {}\n",
    "\n",
    "    the_splits: Iterable[np.ndarray, np.ndarray] = []\n",
    "\n",
    "    child_splits: int = max(1, kfold_splits-1)\n",
    "    child_kf: Union[KFold, Iterable[Tuple[np.ndarray, np.ndarray]]] = KFold(n_splits=child_splits, shuffle=False)\n",
    "\n",
    "    if learn_classes is None:\n",
    "\n",
    "        using_class_weights = False\n",
    "\n",
    "        my_kf: KFold = KFold(\n",
    "            n_splits=kfold_splits,\n",
    "            shuffle=True,\n",
    "            random_state=nested_rng_generator\n",
    "        )\n",
    "        the_splits = my_kf.split(learn_data, learn_targets)\n",
    "\n",
    "    else:\n",
    "\n",
    "        my_kf: StratifiedKFold = StratifiedKFold(\n",
    "            n_splits=kfold_splits,\n",
    "            shuffle=True,\n",
    "            random_state=nested_rng_generator\n",
    "        )\n",
    "\n",
    "        the_splits = my_kf.split(np.zeros_like(learn_classes), learn_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i, (train_indices, test_indices) in enumerate(the_splits, 1):\n",
    "        print(f\"-- {i}/{kfold_splits} start --\")\n",
    "        try:\n",
    "\n",
    "\n",
    "            if learn_classes is not None:\n",
    "                child_kf = [\n",
    "                    i for i in StratifiedKFold(\n",
    "                        n_splits=child_splits,\n",
    "                        shuffle=False\n",
    "                    ).split(\n",
    "                        X = np.zeros_like(train_indices),\n",
    "                        y = learn_classes[train_indices]\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            if using_class_weights:\n",
    "\n",
    "                train_classes: np.ndarray = np.take(learn_classes, train_indices)\n",
    "\n",
    "                train_classes = train_classes / np.sum(train_classes)\n",
    "\n",
    "                test_classes: np.ndarray = np.take(learn_classes, test_indices)\n",
    "                test_classes = test_classes / np.sum(test_classes)\n",
    "\n",
    "                current_search: HalvingGridSearchCV = halving_grid_searcher(\n",
    "                    regressor,\n",
    "                    param_grid,\n",
    "                    learn_data[train_indices],\n",
    "                    learn_targets[train_indices],\n",
    "                    child_kf,\n",
    "                    class_weights = train_classes,\n",
    "                    resource=resource\n",
    "                )\n",
    "\n",
    "                current_score: float = current_search.score(\n",
    "                    learn_data[test_indices],\n",
    "                    learn_targets[test_indices]\n",
    "                )\n",
    "\n",
    "                h_grid_search_dicts[current_search] = current_score\n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "                current_search: HalvingGridSearchCV = halving_grid_searcher(\n",
    "                    regressor,\n",
    "                    param_grid,\n",
    "                    learn_data[train_indices],\n",
    "                    learn_targets[train_indices],\n",
    "                    child_kf,\n",
    "                    resource=resource\n",
    "                )\n",
    "\n",
    "                current_score: float = current_search.score(\n",
    "                    learn_data[test_indices],\n",
    "                    learn_targets[test_indices]\n",
    "                )\n",
    "\n",
    "                h_grid_search_dicts[current_search] = current_score\n",
    "\n",
    "            print(f\"-- {i}/{kfold_splits} done. Score: {current_score} --\")\n",
    "\n",
    "        except NotFittedError as e:\n",
    "            print(\"oh no! there was a not fitted error!\", sys.stderr)\n",
    "            print(e, sys.stderr)\n",
    "            print(traceback.format_exc(), sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return h_grid_search_dicts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('scaler', QuantileTransformer()),\n                ('learner', RandomForestRegressor())])",
      "text/html": "<style>#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc {color: black;background-color: white;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc pre{padding: 0;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-toggleable {background-color: white;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-estimator:hover {background-color: #d4ebff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-item {z-index: 1;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-parallel-item:only-child::after {width: 0;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-be0d5e06-6e2f-461d-9367-6ae264d369bc div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-be0d5e06-6e2f-461d-9367-6ae264d369bc\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, QuantileTransformer()),\n                (&#x27;learner&#x27;, RandomForestRegressor())])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"929edd1f-c607-4376-93bf-c542aadb1e1a\" type=\"checkbox\" ><label for=\"929edd1f-c607-4376-93bf-c542aadb1e1a\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, QuantileTransformer()),\n                (&#x27;learner&#x27;, RandomForestRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bea69f03-9fdd-4383-9155-5e75f020d6e2\" type=\"checkbox\" ><label for=\"bea69f03-9fdd-4383-9155-5e75f020d6e2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuantileTransformer</label><div class=\"sk-toggleable__content\"><pre>QuantileTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"27a688bf-e567-4c0d-bb1f-d918859504e6\" type=\"checkbox\" ><label for=\"27a688bf-e567-4c0d-bb1f-d918859504e6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\",QuantileTransformer()),\n",
    "        (\"learner\",RandomForestRegressor())\n",
    "        #(\"learner\",ARDRegression())\n",
    "        #(\"learner\",AdaBoostRegressor(base_estimator=ARDRegression()))\n",
    "        #(\"learner\",LinearRegression())\n",
    "    ]\n",
    ")\n",
    "learner = fpipeline[\"learner\"]\n",
    "fpipeline.fit(ihdp_learn_df_x.to_numpy(), ihdp_learn_df_y.to_numpy())\n",
    "\n",
    "fpipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7616532820515798"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpipeline.score(ihdp_validation_factual_df_x.to_numpy(), ihdp_validation_factual_df_y.to_numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 1/6 start --\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 25\n",
      "max_resources_: 100\n",
      "aggressive_elimination: True\n",
      "factor: 4\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 720\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 180\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 45\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 12\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 100\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "-- 1/6 done. Score: 0.7342262697236708 --\n",
      "-- 2/6 start --\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 25\n",
      "max_resources_: 100\n",
      "aggressive_elimination: True\n",
      "factor: 4\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 720\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 180\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 45\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 12\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 100\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "-- 2/6 done. Score: 0.7770766926678997 --\n",
      "-- 3/6 start --\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 25\n",
      "max_resources_: 100\n",
      "aggressive_elimination: True\n",
      "factor: 4\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 720\n",
      "n_resources: 25\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_forest_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_halving_grid_searcher(\n",
    "    RandomForestRegressor(criterion=\"squared_error\"),\n",
    "    {\n",
    "        #\"regressor__n_estimators\": [75,100,125],\n",
    "        \"regressor__min_samples_split\": [2,4,6,8],\n",
    "        \"regressor__min_impurity_decrease\": [0, *np.geomspace(0.00001,0.2,6)[1:]],\n",
    "        \"regressor__max_features\": [None,\"sqrt\",\"log2\",1,2],\n",
    "        #\"regressor__oob_score\": [False, True],\n",
    "        \"regressor__ccp_alpha\": [0, *np.geomspace(0.00001,0.2,6)[1:]]\n",
    "    },\n",
    "    ihdp_learn_df_x.values,\n",
    "    ihdp_learn_df_y.values,\n",
    "    learn_classes = ihdp_learn_df_x[\"t\"].to_numpy(),\n",
    "    kfold_splits=6,\n",
    "    resource=\"regressor__n_estimators\"\n",
    "    #classes_ndarray=ihdp_learn_df_x[\"t\"].to_numpy()\n",
    ")\n",
    "\n",
    "rf_searched: HalvingGridSearchCV = max(\n",
    "    random_forest_searched_dict.keys(),\n",
    "    key=lambda k: random_forest_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_rf: Tuple[HalvingGridSearchCV, float] = (\n",
    "    rf_searched,\n",
    "    random_forest_searched_dict[rf_searched]\n",
    ")\n",
    "\n",
    "print(best_rf)\n",
    "\n",
    "print(best_rf[1])\n",
    "\n",
    "\n",
    "rf_searched.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ard_iter: List[int] = [200,300,400]\n",
    "ard_tol: List[float] = [1e-2, 1e-3, 1e-4]\n",
    "ard_alpha_lambda: List[float] = [1e-5, 1e-6, 1e-7]\n",
    "ard_thresh_lambda: List[float] = [1e3, 1e4, 1e5]\n",
    "\n",
    "\n",
    "\n",
    "ard_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_halving_grid_searcher(\n",
    "    ARDRegression(),\n",
    "    {\n",
    "        #\"regressor__n_iter\": ard_iter,\n",
    "        \"regressor__tol\": ard_tol,\n",
    "        \"regressor__alpha_1\" : ard_alpha_lambda,\n",
    "        \"regressor__alpha_2\" : ard_alpha_lambda,\n",
    "        \"regressor__lambda_1\" : ard_alpha_lambda,\n",
    "        \"regressor__lambda_2\" : ard_alpha_lambda,\n",
    "        \"regressor__threshold_lambda\": ard_thresh_lambda\n",
    "    },\n",
    "    ihdp_learn_df_x.values,\n",
    "    ihdp_learn_df_y.values,\n",
    "    learn_classes = ihdp_learn_df_x[\"t\"].to_numpy(),\n",
    "    kfold_splits=6,\n",
    "    resource=\"regressor__n_iter\"\n",
    ")\n",
    "\n",
    "ard_searched: HalvingGridSearchCV = max(\n",
    "    ard_searched_dict.keys(),\n",
    "    key=lambda k: ard_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_ard: Tuple[HalvingGridSearchCV, float] = (\n",
    "    ard_searched,\n",
    "    ard_searched_dict[ard_searched]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(best_ard)\n",
    "\n",
    "print(best_ard[1])\n",
    "\n",
    "ard_searched.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "adaboost_estimators: List[int] = [40, 50, 60]\n",
    "adaboost_learn_rate: List[float] = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "adaboost_loss: List[str]= [\"linear\", \"square\", \"exponential\"]\n",
    "\n",
    "rf_ada_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_halving_grid_searcher(\n",
    "    AdaBoostRegressor(\n",
    "        random_state = _seed,\n",
    "        base_estimator = Pipeline(\n",
    "            steps = [i for i in rf_searched.best_estimator_.named_steps.items()]\n",
    "        )\n",
    "    ),\n",
    "    {\n",
    "        #\"regressor__base_estimator\": [\n",
    "        #    Pipeline(\n",
    "        #        steps = [i for i in rf.best_estimator_.named_steps.items()]\n",
    "        #    ) for rf in random_forest_searched_dict.keys()\n",
    "        #],\n",
    "        #\"regressor__n_estimators\": adaboost_estimators,\n",
    "        \"regressor__learning_rate\": adaboost_learn_rate,\n",
    "        \"regressor__loss\": adaboost_loss\n",
    "    },\n",
    "    ihdp_learn_df_x.values,\n",
    "    ihdp_learn_df_y.values,\n",
    "    learn_classes = ihdp_learn_df_x[\"t\"].to_numpy(),\n",
    "    kfold_splits=6,\n",
    "    resource=\"regressor__n_estimators\"\n",
    ")\n",
    "\n",
    "rf_ada_searched: HalvingGridSearchCV = max(\n",
    "    rf_ada_searched_dict.keys(),\n",
    "    key=lambda k: rf_ada_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_rf_ada: Tuple[HalvingGridSearchCV, float] = (\n",
    "    rf_ada_searched,\n",
    "    rf_ada_searched_dict[rf_ada_searched]\n",
    ")\n",
    "\n",
    "\n",
    "print(best_rf_ada)\n",
    "\n",
    "print(best_rf_ada[1])\n",
    "\n",
    "rf_ada_searched.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ard_ada_searched_dict: Dict[\n",
    "    HalvingGridSearchCV, float\n",
    "] = nested_halving_grid_searcher(\n",
    "    AdaBoostRegressor(\n",
    "        random_state=_seed,\n",
    "        base_estimator=Pipeline(\n",
    "            steps = [i for i in ard_searched.best_estimator_.named_steps.items()]\n",
    "        )\n",
    "    ),\n",
    "    {\n",
    "        #\"regressor__base_estimator\": [\n",
    "        #    Pipeline(\n",
    "        #        steps = [i for i in ard.best_estimator_.named_steps.items()]\n",
    "        #    ) for ard in ard_searched_dict.keys()\n",
    "        #],\n",
    "        #\"regressor__n_estimators\": adaboost_estimators,\n",
    "        \"regressor__learning_rate\": adaboost_learn_rate,\n",
    "        \"regressor__loss\": adaboost_loss\n",
    "    },\n",
    "    ihdp_learn_df_x.values,\n",
    "    ihdp_learn_df_y.values,\n",
    "    learn_classes = ihdp_learn_df_x[\"t\"].to_numpy(),\n",
    "    kfold_splits=6,\n",
    "    resource=\"regressor__n_estimators\"\n",
    ")\n",
    "\n",
    "ard_ada_searched: HalvingGridSearchCV = max(\n",
    "    ard_ada_searched_dict.keys(),\n",
    "    key=lambda k: ard_ada_searched_dict[k]\n",
    ")\n",
    "\n",
    "best_ard_ada: Tuple[HalvingGridSearchCV, float] = (\n",
    "    ard_ada_searched,\n",
    "    ard_ada_searched_dict[ard_ada_searched]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(best_ard_ada)\n",
    "\n",
    "print(best_ard_ada[1])\n",
    "\n",
    "ard_ada_searched.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(init=True, eq=True, repr=True, frozen=True)\n",
    "class RegressorInfoDataclass:\n",
    "\n",
    "    regressor: Pipeline\n",
    "\n",
    "    t0_t1_ite_predictions: pd.DataFrame\n",
    "\n",
    "    t0_r2: float\n",
    "    t1_r2: float\n",
    "    ite_r2: float\n",
    "\n",
    "    def __lt__(self, other: \"RegressorInfoDataclass\") -> bool:\n",
    "        if self.ite_r2 < other.ite_r2:\n",
    "            return True\n",
    "        if self.ite_r2 == other.ite_r2:\n",
    "            # basically the r2 scores for t1 and t0 are shifted down to\n",
    "            # have an upper limit of -1, then the products of the\n",
    "            # shifted r2 scores are found.\n",
    "            # higher product = worse r2 scores: counted as 'less than'\n",
    "            # returns true if this object's combined r2 is worse than other.\n",
    "            return (\n",
    "               (self.t0_r2-2) * (self.t1_r2-2)\n",
    "            ) > (\n",
    "                (other.t0_r2-2) * (other.t1_r2-2)\n",
    "            )\n",
    "        return False\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def make(\n",
    "            cls,\n",
    "            the_regressor: Pipeline,\n",
    "            the_x_data: pd.DataFrame,\n",
    "            true_t0_t1_ite: pd.DataFrame\n",
    "    ) -> \"RegressorInfoDataclass\":\n",
    "\n",
    "        x_with_predictions: pd.DataFrame = the_x_data.copy()\n",
    "\n",
    "        x_t_data: pd.DataFrame = the_x_data.copy()\n",
    "\n",
    "        x_t_data[\"t\"] = 0\n",
    "\n",
    "        x_with_predictions[\"t0\"] = the_regressor.predict(x_t_data.to_numpy())\n",
    "\n",
    "        t0_r2_score: float = r2_score(true_t0_t1_ite[\"t0\"], x_with_predictions[\"t0\"])\n",
    "\n",
    "        x_t_data[\"t\"] = 1\n",
    "\n",
    "        x_with_predictions[\"t1\"] = the_regressor.predict(x_t_data.to_numpy())\n",
    "\n",
    "        t1_r2_score: float = r2_score(true_t0_t1_ite[\"t1\"], x_with_predictions[\"t1\"])\n",
    "\n",
    "        x_with_predictions[\"ite\"] = x_with_predictions[\"t1\"] - x_with_predictions[\"t0\"]\n",
    "\n",
    "        ite_r2_score: float = r2_score(true_t0_t1_ite[\"ite\"], x_with_predictions[\"ite\"])\n",
    "\n",
    "        return RegressorInfoDataclass(\n",
    "            regressor=the_regressor,\n",
    "            t0_t1_ite_predictions=x_with_predictions,\n",
    "            t0_r2  = t0_r2_score,\n",
    "            t1_r2  = t1_r2_score,\n",
    "            ite_r2 = ite_r2_score\n",
    "        )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_best_classifiers: List[RegressorInfoDataclass] = [\n",
    "    RegressorInfoDataclass.make(\n",
    "        reg,\n",
    "        ihdp_factuals,\n",
    "        ihdp_full\n",
    "    ) for reg in [\n",
    "        rf_searched.best_estimator_,\n",
    "        ard_searched.best_estimator_,\n",
    "        rf_ada_searched.best_estimator_,\n",
    "        ard_searched.best_estimator_\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}