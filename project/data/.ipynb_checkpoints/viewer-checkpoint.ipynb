{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Requirements to run this:\n",
    "\n",
    "* matplotlib\n",
    "* numpy\n",
    "* pandas\n",
    "* seaborn\n",
    "* patchworklib\n",
    "    * `pip install patchworklib` (makes some bits of seaborn less awkward)\n",
    "    * https://github.com/ponnhide/patchworklib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patchworklib in \\\\isslx111.essex.ac.uk\\rl18730\\msc\\ce888\\ce888\\venv\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.4 in c:\\python3\\lib\\site-packages (from patchworklib) (3.4.2)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\python3\\lib\\site-packages (from patchworklib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\python3\\lib\\site-packages (from patchworklib) (1.19.5)\n",
      "Requirement already satisfied: dill in \\\\isslx111.essex.ac.uk\\rl18730\\msc\\ce888\\ce888\\venv\\lib\\site-packages (from patchworklib) (0.3.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\python3\\lib\\site-packages (from matplotlib>=3.4->patchworklib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python3\\lib\\site-packages (from matplotlib>=3.4->patchworklib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python3\\lib\\site-packages (from matplotlib>=3.4->patchworklib) (8.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python3\\lib\\site-packages (from matplotlib>=3.4->patchworklib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python3\\lib\\site-packages (from matplotlib>=3.4->patchworklib) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\python3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.4->patchworklib) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\python3\\lib\\site-packages (from pandas>=0.24->patchworklib) (2021.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '\\\\isslx111.essex.ac.uk\\rl18730\\MSc\\CE888\\ce888\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install patchworklib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'patchworklib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13592/158196225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdoctest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpatchworklib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'patchworklib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.colorbar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from typing import Iterable, Union, Dict, List, Tuple, Optional, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import doctest\n",
    "import patchworklib as pw\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from collections.abc import Hashable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Loading the data\n",
    "\n",
    "Before I can start analysing the data, I need to actually load it.\n",
    "These functions (below) are intended to make that job a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1: Some utility functions, to allow me to load the data in a way that makes some sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def npz_to_dict(\n",
    "        npz_filename: str,\n",
    "        **kwargs\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Puts the given NPZ file into a dictionary of {table name, ndarray}.\n",
    "    :param npz_filename: the filename of the NPZ file that we want to put into a dictionary.\n",
    "    :param kwargs: kwargs from https://numpy.org/doc/stable/reference/generated/numpy.load.html#numpy.load.\n",
    "    DO NOT INCLUDE A 'mmap_mode' KWARG!!!\n",
    "    :return: The data from the given npz file in a dictionary.\n",
    "    \"\"\"\n",
    "    data_dict: Dict[str, np.ndarray] = {}\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    with np.load(npz_filename, mmap_mode=\"r\",**kwargs) as npz:\n",
    "        for f in npz.files:\n",
    "            data_dict[f] = npz[f]\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def turn_01_columns_into_int(\n",
    "        dataframe_to_edit: DataFrame,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Finds all of the columns that just contain values of 0 and 1,\n",
    "    and converts all of those columns to ints.\n",
    "    MODIFIES THE GIVEN DATAFRAME!\n",
    "    :param dataframe_to_edit: the dataframe that is being edited\n",
    "    :return: The modified dataframe.\n",
    "    DOES NOT COPY THE GIVEN ORIGINAL DATAFRAME.\n",
    "\n",
    "    >>> before: DataFrame = DataFrame.from_dict(data={\"int01\":[0,1,1,0],\"flt01\":[0.0, 1.0, 0.0, 1.0], \"intNo\": [-1,0,1,2], \"fltNo\":[-1.0, 0.0, 1.0, 2.0], \"intNan\": [0,1,None,0], \"fltNan\":[0.0,1.0,None,0.0]})\n",
    "    >>> before_types = before.dtypes.values\n",
    "    >>> after: DataFrame = turn_01_columns_into_int(before.copy())\n",
    "    >>> after_types = after.dtypes.values\n",
    "    >>> print(after_types[0])\n",
    "    uint8\n",
    "    >>> print(after_types[1])\n",
    "    uint8\n",
    "    >>> print(f\"{before_types[2] == after_types[2]} {before_types[3] == after_types[3]} {before_types[4] == after_types[4]} {before_types[5] == after_types[5]}\")\n",
    "    True True True True\n",
    "    \"\"\"\n",
    "    for c in dataframe_to_edit.columns:\n",
    "        if dataframe_to_edit[c].dtype == np.uint8:\n",
    "            continue\n",
    "        if dataframe_to_edit[c].isin([0,1]).all():\n",
    "            dataframe_to_edit[c] = dataframe_to_edit[c].astype(np.uint8)\n",
    "    return dataframe_to_edit\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doctest.run_docstring_examples(turn_01_columns_into_int, globals())\n",
    "\n",
    "def x_to_dataframe(\n",
    "        x_data: np.ndarray,\n",
    "        row_major = True,\n",
    "        x_prefix: str = \"x_\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'x' ndarray into a pandas dataframe.\n",
    "    :param x_data: the ndarray containing all of the data\n",
    "    :param row_major: is this ndarray held in row-major order? [[item a data], [item b data], ... ]\n",
    "    :param x_prefix: prefix to put on the names of all of the x columns\n",
    "    :return: a dataframe holding the given x data.\n",
    "    \"\"\"\n",
    "    if row_major:\n",
    "        x_data: np.ndarray = x_data.T\n",
    "    x_df: DataFrame = pd.DataFrame.from_dict({f\"{x_prefix}{i}\": x_data[i] for i in range(x_data.shape[0])})\n",
    "\n",
    "    return turn_01_columns_into_int(x_df)\n",
    "\n",
    "\n",
    "\n",
    "def add_everything_but_x_to_copy_of_dataframe(\n",
    "        original_df: DataFrame,\n",
    "        the_data_dict: Dict[str, np.ndarray],\n",
    "        dont_add: Union[str, Iterable[str]]=frozenset('x')\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds everything in the npz file apart from the given 'dont_add'\n",
    "    tables to the dataframe.\n",
    "    Assumes that these other tables have the same shape of (whatever, 1).\n",
    "    :param original_df: the original dataframe that shall be copied and have stuff added to it.\n",
    "    :param the_data_dict: The data file with the data to be added to the DataFrame\n",
    "    :param dont_add: the identifier(s) of the columns that must not be added to the DataFrame.\n",
    "    :return: a copy of the original dataframe, with the data from every table BESIDES the 'dont add' tables\n",
    "    from the given file added to it.\n",
    "    \"\"\"\n",
    "\n",
    "    the_df = original_df.copy()\n",
    "    if dont_add in the_data_dict.keys():\n",
    "        dont_add = [dont_add]\n",
    "    for k, v in the_data_dict.items():\n",
    "        if k in dont_add:\n",
    "            continue\n",
    "        the_df[k] = turn_01_columns_into_int(DataFrame(v))\n",
    "    return the_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1.2: And some utility functions for producing graphs based on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bin_finder(n: int, default_bins: int) -> int:\n",
    "\n",
    "    if n <= default_bins:\n",
    "        return n\n",
    "\n",
    "    biggest_possible = np.floor(np.sqrt(n)).astype(int)\n",
    "\n",
    "    potentials = np.arange(biggest_possible, stop=0, step=-1)\n",
    "    divisor_indices = np.flatnonzero(n % potentials==0)\n",
    "\n",
    "    print(potentials[divisor_indices])\n",
    "    if divisor_indices.size > 0:\n",
    "        print(f\"{n} -> {n // potentials[divisor_indices[0]]}\")\n",
    "        return min(default_bins, n // potentials[divisor_indices[0]])\n",
    "    return default_bins\n",
    "\n",
    "\n",
    "def squareish_w_h(div_this: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Attempts to make a shape not entirely unlike a square\n",
    "    :param div_this: int to divide\n",
    "    :return: square-ish w/h\n",
    "    \"\"\"\n",
    "    if np.sqrt(div_this) % 1 == 0:\n",
    "        res = np.floor(np.sqrt(div_this))\n",
    "        return res, res\n",
    "    elif div_this & 1 == 0: # odd number\n",
    "        return fallback_w_h(div_this)\n",
    "    else:\n",
    "        # if even, we work out which binary rectangle works best basically\n",
    "        a: int = 1\n",
    "        b: int = div_this\n",
    "        while b > a and b & 1 == 0:\n",
    "            b //= 2\n",
    "            a *= 2\n",
    "        if max(a, b) > min(a, b) * 2:\n",
    "            return fallback_w_h(div_this)\n",
    "        else:\n",
    "            return tuple(*sorted((a, b), reverse=True))\n",
    "\n",
    "def fallback_w_h(div_this: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Attempts to make a shape not entirely unlike a square\n",
    "    :param div_this: int to divide\n",
    "    :return: square-ish w/h\n",
    "    \"\"\"\n",
    "    h = np.floor(np.sqrt(div_this))\n",
    "    # get something relatively close to the square\n",
    "    w = div_this//h\n",
    "\n",
    "    i_w = w < h\n",
    "    if w * h < div_this:\n",
    "        if i_w:\n",
    "            w += 1\n",
    "        else:\n",
    "            h += 1\n",
    "        i_w = not i_w\n",
    "    return w, h\n",
    "\n",
    "def jointplot_maker(df: DataFrame, xlabel: str, ylabel: str, tlabel: str, wh: int = 5):\n",
    "\n",
    "    pw.param[\"margin\"] = 0.1\n",
    "\n",
    "    ax1 = pw.Brick(f\"{xlabel}_ax1\",figsize=(wh, wh/4))\n",
    "    sns.histplot(data=df, x=xlabel, hue=tlabel, kde=True, ax=ax1)\n",
    "    ax1.spines[\"top\"].set_visible(False)\n",
    "    ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax2 = pw.Brick(f\"{xlabel}_ax2\",figsize=(wh/4, wh))\n",
    "    sns.histplot(data=df, y=ylabel, hue=tlabel, kde=True, ax=ax2)\n",
    "    ax2.spines[\"top\"].set_visible(False)\n",
    "    ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax3 = pw.Brick(f\"{xlabel}_ax3\",figsize=(3*wh/4, 3*wh/4))\n",
    "    sns.displot(data=df, x=xlabel, y=ylabel, hue=tlabel, ax=ax3)\n",
    "    sns.scatterplot(data=df, x=xlabel, y=ylabel, hue=tlabel, ax=ax3, alpha=.5)\n",
    "\n",
    "    ax1.set_xlim(ax3.get_xlim())\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_xlabel(\"\")\n",
    "\n",
    "    ax2.set_ylim(ax3.get_ylim())\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    ax13 = ax1/ax3\n",
    "    ax132 = ax13[f\"{xlabel}_ax3\"]|ax2\n",
    "\n",
    "    ax132.case.set_title(f\"Distribution for {xlabel}\")\n",
    "    return ax132\n",
    "\n",
    "\n",
    "def viz2(df: DataFrame, xlabels: List[Any], ylabel: Any, tlabel: Any, fig_subject: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Produces a series of graphs based on the given data\n",
    "    :param df: the dataframe\n",
    "    :param xlabels: column names for T\n",
    "    :param ylabel: column name for Y\n",
    "    :param tlabel: column name for T\n",
    "    :param fig_subject: words for what the source of the data being shown is\n",
    "    :return: a figure showing the input data as a series of histograms\n",
    "    \"\"\"\n",
    "\n",
    "    y_series: pd.Series = df[ylabel]\n",
    "    t_series: pd.Series = df[tlabel]\n",
    "\n",
    "    Y: np.ndarray = y_series.values\n",
    "    T: np.ndarray = t_series.values\n",
    "\n",
    "    bins: int = bin_finder(Y.size, 20)\n",
    "\n",
    "    if fig_subject is None:\n",
    "        fig_subject = \"\"\n",
    "    else:\n",
    "        fig_subject = f\" {fig_subject}\"\n",
    "\n",
    "    print(xlabels)\n",
    "\n",
    "    x_count: int = len(xlabels)\n",
    "\n",
    "    w, h = squareish_w_h(x_count)\n",
    "\n",
    "    sns.FacetGrid = sns.FacetGrid(data=df, col=\"\")\n",
    "\n",
    "    #fig: plt.Figure = plt.Figure(\n",
    "    #    figsize=(w*3, h*3)\n",
    "    #)\n",
    "\n",
    "    #axes: Tuple[plt.Axes] = fig.subplots(\n",
    "    #    nrows=h, ncols=w, squeeze=True\n",
    "    #)\n",
    "\n",
    "    figrows = []\n",
    "\n",
    "    current_row = -1\n",
    "\n",
    "    for i in range(x_count):\n",
    "\n",
    "        this_plot = jointplot_maker(\n",
    "            df,\n",
    "            xlabels[i],\n",
    "            ylabel,\n",
    "            tlabel\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        if i % w == 0:\n",
    "            figrows.append(this_plot)\n",
    "            current_row += 1\n",
    "        else:\n",
    "            figrows[current_row] |= this_plot\n",
    "\n",
    "    the_fig = figrows[0]\n",
    "    for i in range(1, len(figrows)):\n",
    "        the_fig /= figrows[i]\n",
    "\n",
    "    the_figure: plt.Figure = figrows.savefig()\n",
    "    the_figure.suptitle(f\"Joint graphs for {fig_subject}\")\n",
    "    return the_figure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualizer(df: DataFrame, xlabels: List[Any], ylabel: Any, tlabel: Any, fig_subject: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Produces a series of graphs based on the given data\n",
    "    :param df: the dataframe\n",
    "    :param xlabels: column names for T\n",
    "    :param ylabel: column name for Y\n",
    "    :param tlabel: column name for T\n",
    "    :param fig_subject: words for what the source of the data being shown is\n",
    "    :return: a figure showing the input data as a series of histograms\n",
    "    \"\"\"\n",
    "\n",
    "    y_series: pd.Series = df[ylabel]\n",
    "    t_series: pd.Series = df[tlabel]\n",
    "\n",
    "    Y: np.ndarray = y_series.values\n",
    "    T: np.ndarray = t_series.values\n",
    "\n",
    "    bins: int = bin_finder(Y.size, 20)\n",
    "\n",
    "\n",
    "\n",
    "    if fig_subject is None:\n",
    "        fig_subject = \"\"\n",
    "    else:\n",
    "        fig_subject = f\" {fig_subject}\"\n",
    "\n",
    "    print(xlabels)\n",
    "\n",
    "    y_t_str: Tuple[str, str] = (\"Y\", \"T\")\n",
    "    y_t_np: Tuple[np.ndarray, np.ndarray] = (Y, T)\n",
    "    y_t_bins: Tuple[int, int] = (\n",
    "        min(bins,np.unique(Y).shape[0]),\n",
    "        min(bins,np.unique(T).shape[0])\n",
    "    )\n",
    "    \"This will be useful later on.\"\n",
    "\n",
    "    y_treated: np.ndarray = Y[np.argwhere(T==1)]\n",
    "    y_untreated: np.ndarray = Y[np.argwhere(T==0)]\n",
    "\n",
    "    col_count: int = len(xlabels)\n",
    "    fig: plt.Figure = plt.figure(figsize=(24, 1+(4*col_count)))\n",
    "    subfigs: tuple[plt.Figure, ...] = fig.subfigures(nrows=col_count, ncols=1)\n",
    "    for i in range(col_count):\n",
    "\n",
    "        sfig: plt.Figure = subfigs[i]\n",
    "        label: str = xlabels[i]\n",
    "        x_data: np.ndarray = df[label].to_numpy()\n",
    "\n",
    "\n",
    "        this_bins = bin_finder(df[label].nunique(), 20)\n",
    "\n",
    "        sfig.suptitle(f\"Data for {label}\")\n",
    "        sfigs: tuple[plt.Figure, plt.Figure] = sfig.subfigures(nrows=1,ncols=3,width_ratios=[1,1,3])\n",
    "\n",
    "        hist1: plt.Figure = sfigs[0]\n",
    "        hist1.suptitle(f\"{label} quantities\")\n",
    "        hist_x_plt: plt.Axes = hist1.add_subplot()\n",
    "        sns.histplot(data = x_data, stat=\"frequency\", bins = this_bins, kde=True, ax=hist_x_plt)\n",
    "        hist_x_plt.set_xlabel(f\"{label}\")\n",
    "        hist_x_plt.set_ylabel(\"quantity\")\n",
    "\n",
    "        hist2: plt.Figure = sfigs[1]\n",
    "\n",
    "        hist2_plt: plt.Axes = hist2.add_subplot()\n",
    "        sns.kdeplot(\n",
    "                data=df[[label, ylabel, tlabel]], x=label,y=ylabel,\n",
    "                ax=hist2_plt, legend=True, hue=tlabel, fill=True,\n",
    "                alpha=.5, #kind=\"kde\"\n",
    "        )\n",
    "        hist2_plt.set_title(f\"{label} and Y for treatment/control\")\n",
    "        hist2_plt.set_xlabel(f\"{label}\")\n",
    "        hist2_plt.set_ylabel(f\"outcome given {label}\")\n",
    "\n",
    "        scatters: plt.Figure = sfigs[2]\n",
    "\n",
    "        x_treated: np.ndarray = x_data[np.argwhere(T==1)]\n",
    "        x_untreated: np.ndarray = x_data[np.argwhere(T==0)]\n",
    "\n",
    "        #hists.suptitle(f\"More histograms for {label}\")\n",
    "        #hist_axes: Tuple[plt.Axes, plt.Axes, plt.Axes] = hists.subplots(nrows=1, ncols=2, sharex=\"all\")\n",
    "\n",
    "\n",
    "        scatters.suptitle(f\"Scatter graphs for {label}\")\n",
    "        ax0, ax1, ax2 = scatters.subplots(nrows=1, ncols=3, sharex=\"all\",sharey=\"all\")\n",
    "        ax0.set_xlabel(f\"{label}\")\n",
    "        ax0.set_ylabel(f\"Outcome from {label}\")\n",
    "        ax0.set_title(\"both\")\n",
    "        ax1.set_title(\"treated only\")\n",
    "        ax2.set_title(\"control only\")\n",
    "        ax0.scatter(x_treated, y_treated, c = \"g\", label = \"Treated\")\n",
    "        ax1.scatter(x_treated, y_treated, c = \"g\")\n",
    "        ax0.scatter(x_untreated, y_untreated, c = \"r\", label = \"Control\")\n",
    "        ax2.scatter(x_untreated, y_untreated, c = \"r\")\n",
    "        ax0.legend()\n",
    "\n",
    "    fig.suptitle(f\"X, Y, T visualizations all dimensions of X in{fig_subject}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2: Loading the IHPD dataset\n",
    "\n",
    "Firstly, I need to load the dataset, and see what data is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ihdp_dict: Dict[str, np.ndarray] = npz_to_dict(\"ihdp.npz\", allow_pickle = False)\n",
    "\n",
    "\n",
    "for k,v in ihdp_dict.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2.1: Extracting the 'x' column, and moving it to a dataframe\n",
    "\n",
    "It looks like the 'x' data is a table of 747 rows and 25 columns.\n",
    "This, by itself, is a bit difficult to digest, so I shall move that\n",
    "into a dataframe, making it a bit easier to analyse (for now at least).\n",
    "\n",
    "The following two code cells are just outputting an overview of the 'x'\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ihdp_df_x: DataFrame = x_to_dataframe(ihdp_dict['x'])\n",
    "\"\"\"Dataframe holding the 'x' data for the IHDP dataset\"\"\"\n",
    "\n",
    "ihdp_df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(ihdp_df_x.info())\n",
    "\n",
    "\n",
    "\n",
    "ihdp_df_x.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(ihdp_df_x.nunique())\n",
    "\n",
    "print(\"\\nThe actual value counts for x_3\")\n",
    "print(ihdp_df_x[\"x_3\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at `x_3`, and how all of the values in it are one of four distinct values,\n",
    " all of which are painfully close to being int values, I **really** want to just\n",
    " convert them into raw ints, which would be rather trivial to do,\n",
    " as follows (converting -0.879606 to 0, 0.161703 to 1, 1.203011 to 2,  2.244320 to 3, etc):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def x3_to_int(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    x3 = df[\"x_3\"]\n",
    "    x3_vals = [*x3.value_counts().keys()]\n",
    "    x3_vals.sort()\n",
    "    x3_diff = x3_vals[2] - x3_vals[1]\n",
    "    x3 += x3_diff\n",
    "    x3 /= x3_diff\n",
    "    df[\"x_3\"] = x3.astype(np.int8)\n",
    "    return df\n",
    "```\n",
    "\n",
    "However, I will refrain from doing this. This is because I don't actually know what\n",
    "the data in `x_3` is supposed to be. If I knew, with certainty, that `x_3`\n",
    "would only ever be one of those for values (or, failing that, would always\n",
    "be a value from a continuous range of values with a consistent\n",
    "gap of `1.041308` between each value), I would be applying that bit of\n",
    "scaling to `x_3`. But, as mentioned earlier, I don't. So I can't. Which is a shame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2.2: Extracting the rest of the IHDP dataset, before adding it to a dataframe\n",
    "\n",
    "I shall copy the X data into a new dataframe (ensuring that I retain a backup copy of the dataframe holding\n",
    "just the X data), and add the rest of the IHDP data to that new dataframe,\n",
    "meaning that I'll be able to get a full overview of IHDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ihdp_df: DataFrame = add_everything_but_x_to_copy_of_dataframe(\n",
    "    ihdp_df_x.copy(),\n",
    "    ihdp_dict,\n",
    "    \"x\"\n",
    ")\n",
    "\"\"\"Dataframe holding the entirety of the IHDP dataset\"\"\"\n",
    "\n",
    "ihdp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"info about the full dataset\")\n",
    "print(ihdp_df.info())\n",
    "\n",
    "print(\"\\ntreatment info for the full dataset\")\n",
    "print(ihdp_df[\"t\"].value_counts())\n",
    "\n",
    "print(\"\\ndescribing the dataset\")\n",
    "ihdp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2.3: Visualizing the IHDP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#visualizer(ihdp_df, [*ihdp_df_x.columns] , \"yf\", \"t\",fig_subject=\"IHDP\").show()\n",
    "\n",
    "viz2(ihdp_df, [*ihdp_df_x.columns], \"yf\",\"t\", fig_subject=\"IHDP\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2.4: Discussing the IHDP dataset\n",
    "\n",
    "We can see that most of the X measures in this dataset are boolean 'yes/no' values (with only x values 0-5\n",
    "being continuous values). This implies that we don't need to perform any scaling on these latter 19 values to keep\n",
    "them in a manageable range, however, we may need to consider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3: Loading the JOBS dataset\n",
    "\n",
    "As above, but for the JOBS dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jobs_dict: Dict[str, np.ndarray] = npz_to_dict(\"jobs.npz\")\n",
    "\n",
    "\n",
    "for k,v in jobs_dict.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3.1: Loading the X data from JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jobs_df_x: DataFrame = x_to_dataframe(jobs_dict['x'])\n",
    "\n",
    "jobs_df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(jobs_df_x.info())\n",
    "\n",
    "jobs_df_x.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(jobs_df_x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3.2: Loading the remaining data from JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jobs_df: DataFrame = add_everything_but_x_to_copy_of_dataframe(\n",
    "    jobs_df_x.copy(),\n",
    "    jobs_dict,\n",
    "    \"x\"\n",
    ")\n",
    "\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(jobs_df.info())\n",
    "\n",
    "jobs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3.3: Discussing the JOBS dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
